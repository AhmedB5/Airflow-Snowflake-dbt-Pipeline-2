#  Airflowâ€“Snowflakeâ€“dbt Pipeline

##  Overview  
A data pipeline built with **Airflow**, **Snowflake**, and **dbt** running in **Docker**.  
The pipeline loads raw CSV data into Snowflake and transforms it using dbt models, macros, and snapshots.

---

## âš™ï¸ Workflow  
1. **Extract & Load:** Load data from CSV into **Snowflake** tables.  
2. **Transform:** Use **dbt models** and **macros** for data cleaning and transformation.  
3. **Snapshot:** Track historical data changes using **dbt snapshots**.  
4. **Orchestration:** All steps are managed through **Airflow DAGs** in Docker containers.

---

## ğŸ§© Tech Stack  
- **Airflow** â€“ Workflow orchestration  
- **Snowflake** â€“ Cloud data warehouse  
- **dbt** â€“ Data transformation  
- **Docker** â€“ Containerized environment  
- **CSV** â€“ Data source  

---

## ğŸ“ Project Structure  
```
dags/ # Airflow DAGs (airbnb_dag.py, exampledag.py)
dbt_env/ # Virtual environment/config
dbt_project/
â”œâ”€ models/ # dbt models (example, staging)
â”œâ”€ macros/ # Custom dbt macros (handle_blank.sql)
â”œâ”€ snapshots/ # dbt snapshots
â”œâ”€ seeds/ # Seed data if needed
â”œâ”€ tests/ # dbt tests
â””â”€ dbt_project.yml # Main dbt config
include/ # Connection profiles and configs
data/ # Raw CSV files
```

